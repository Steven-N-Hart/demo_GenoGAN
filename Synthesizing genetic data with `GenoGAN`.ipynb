{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read load the annotation TFRecords to get the annotations as well as the number of rows and columns to better understand the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4322, 14)\n"
     ]
    }
   ],
   "source": [
    "annotation_file_name = 'annotations.tfrecords'\n",
    "genotype_file_name = 'cases.tfrecords'\n",
    "\n",
    "num_epochs = 2\n",
    "X_dim   = 60508 # This is how long our vector is.\n",
    "                # 4322 genotypes with  14 annotations each = 60508\n",
    "mb_size = 10    # Minibatch size\n",
    "lr =      0.01  # Learning rate\n",
    "\n",
    "gt_length = 4322 #Use in places that have 784\n",
    "h_dim = 100      #use where you see 100\n",
    "\n",
    "annotations = tf.placeholder(tf.int64, shape=[None, X_dim], name='annotations')\n",
    "ncols = tf.placeholder(tf.int64, shape=[None, 1], name='ncols')\n",
    "nrows = tf.placeholder(tf.int64, shape=[None, 1], name='nrows')\n",
    "\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'annotations': tf.FixedLenFeature([X_dim], tf.int64),\n",
    "              'ncols': tf.FixedLenFeature([1], tf.int64),\n",
    "              'nrows': tf.FixedLenFeature([1], tf.int64),\n",
    "              })\n",
    "    annotations = tf.cast(features['annotations'], tf.int32, name='anno_casting')\n",
    "    ncols = tf.cast(features['ncols'], tf.int32, name='ncol_casting')\n",
    "    nrows = tf.cast(features['nrows'], tf.int32, name='nrow_casting')\n",
    "    return annotations,ncols,nrows\n",
    "\n",
    "\n",
    "def get_all_records():\n",
    "    with tf.Session() as sess:\n",
    "        filename_queue = tf.train.string_input_producer([annotation_file_name])\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        annotations,ncols,nrows = read_and_decode(filename_queue)\n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        try:\n",
    "            annotations,ncols,nrows = sess.run([annotations,ncols,nrows])\n",
    "            return annotations,ncols,nrows\n",
    "\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "annotations,ncols,nrows = get_all_records()\n",
    "\n",
    "# convert list to scalars where appropriate\n",
    "ncols,nrows = ncols[0],nrows[0]\n",
    "\n",
    "annotations = np.reshape(annotations, (nrows,ncols))\n",
    "print(annotations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool.  Now I have a numpy array of annotations as well as the number of variants in my model (n=4322).  I can use these features in the next step.\n",
    "\n",
    "## Building a generator\n",
    "I want to build a generator that takes as input an h_dim-dimensional vector and returns a nrows-dimensional vector of 0, 1, or 2 that correspond to reference, heterozygous, and homozygous alternate, respectively. \n",
    "\n",
    "Before we do that, let's define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator variables\n",
    "# genotypes can only be 0, 1 or 2\n",
    "min_value = 0.   \n",
    "max_value = 3.\n",
    "\n",
    "# Descriminator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'generator/Sigmoid:0' shape=(?, 4322) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev,dtype=tf.float32)\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "# Set variables\n",
    "# Generator Net\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, h_dim])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([h_dim, 128]),dtype=tf.float32)\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]),dtype=tf.float32)\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, gt_length]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[gt_length]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n]).tolist()\n",
    "\n",
    "#def generator(z):\n",
    "#    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "#    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    # Strech values from [0,3)\n",
    "#    G_scaled = tf.div(\n",
    "#       tf.subtract(\n",
    "#          G_log_prob, min_value\n",
    "#       ), \n",
    "#        (max_value - min_value)\n",
    "#    )\n",
    "#    # round to integer\n",
    "#    G_int = tf.cast(tf.round(G_scaled),tf.int32)\n",
    "#    G_transposed = tf.transpose(G_int)\n",
    "#    return(G_transposed)\n",
    "\n",
    "def generator(z):\n",
    "    with tf.name_scope('generator'):\n",
    "        G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "        G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "        G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "        return tf.cast(G_prob,dtype=tf.float32)\n",
    "\n",
    "\n",
    "generator(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a discriminator\n",
    "Now that we have a generator, we need to build a discriminator to dtermine whether it looks realistic or not.  It needs to take in a vector of genotypes (nrows) and output a prediction of 'real' or 'fake'.\n",
    "\n",
    "However, this is also where I want to do something a bit different from other algortihms.  I want the discriminator to have access to something the generator doesn't - the annotations.  Biologicially, this means we are effectively bounding the possible genotypes to what would be expected based on certain parameters like 'protein coding', 'missense prediction', 'allele frequency', etc.  Then, we'll let the network sort out what the appropriate genotype should be allowed for each variant.  So what I want to do is to combine the genotype input (either real or fake) with each of the annotations, and then create a neural network to predict true or false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set variables\n",
    "# Discriminator Net\n",
    "X = tf.placeholder(tf.float32, shape=[None, gt_length])\n",
    "#print('X: {}'.format(X))\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([gt_length,128]))\n",
    "#print('D_W1: {}'.format(D_W1))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128,1]))\n",
    "#print('D_W2: {}'.format(D_W2))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "def discriminator(x):\n",
    "    with tf.name_scope('Descriminator'):\n",
    "        #print('X: {}'.format(x))\n",
    "        #print('D_W1: {}'.format(D_W1))\n",
    "        #print('D_b1: {}'.format(D_b1))\n",
    "        D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "        D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "        D_prob = tf.nn.sigmoid(D_logit)\n",
    "        return D_prob, D_logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read genotypes\n",
    "So far we've: \n",
    " * read through our annotations\n",
    " * built a genotype generator\n",
    " * built a discriminator that combines data from the genotypes\n",
    " \n",
    " but now we need to build a function to read in our genotypes in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode_genotypes(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'label':     tf.FixedLenFeature([], tf.int64),\n",
    "                  'genotypes': tf.FixedLenFeature([gt_length], tf.int64),\n",
    "                 }        \n",
    "    )\n",
    "    \n",
    "    genotypes = tf.cast(features['genotypes'], tf.float32, name='geno_casting')\n",
    "    genotypes = tf.Print(genotypes,[genotypes],'genotypes: ')\n",
    "    genotypes = tf.reshape(genotypes,[gt_length])\n",
    "    return genotypes\n",
    "\n",
    "def inputs(genotype_file_name, mb_size=mb_size, num_epochs=num_epochs):\n",
    "    if not num_epochs:\n",
    "        num_epochs = None\n",
    "    with tf.name_scope('input'):\n",
    "        filename_queue = tf.train.string_input_producer([genotype_file_name], num_epochs=num_epochs)\n",
    "        gt = read_and_decode_genotypes(filename_queue)\n",
    "        #print('gt: {}'.format(gt))\n",
    "        gts = tf.train.shuffle_batch(\n",
    "                [gt],\n",
    "                batch_size=mb_size,\n",
    "                num_threads=2,\n",
    "                capacity=1000 + 3 * mb_size,\n",
    "                min_after_dequeue=1)\n",
    "        gts = tf.Print(gts,[gts],'gts shape: ')\n",
    "        print('gts: {}'.format(gts))\n",
    "    return gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gts: Tensor(\"input/Print_1:0\", shape=(10, 4322), dtype=float32)\n",
      "GT is \n",
      "z_mb: (10, 100)\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-67b9f5b129e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         _, D_loss_curr = sess.run(\n\u001b[1;32m     36\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mD_solver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mGT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mz_mb\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         )\n\u001b[1;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Steven\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Steven\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m   1075\u001b[0m                             \u001b[1;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                             'strings, lists, numpy ndarrays, or TensorHandles.')\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles."
     ]
    }
   ],
   "source": [
    "GT = tf.placeholder(tf.float32, shape=[mb_size, gt_length])\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "#Iterate\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for it in range(10):\n",
    "        GT = inputs(genotype_file_name)\n",
    "        print('GT is '.format(GT))\n",
    "        z_mb = sample_z(mb_size, h_dim)\n",
    "        print('z_mb: {}'.format(np.array(z_mb).shape))\n",
    "        \n",
    "        D_real = discriminator(GT)\n",
    "        D_fake = discriminator(generator(sample_z(mb_size, h_dim)))\n",
    "        #print('D_real: {}'.format(D_real))\n",
    "        #print('D_fake: {}'.format(D_fake))\n",
    "        \n",
    "        _, D_loss_curr = sess.run(\n",
    "            [D_solver, D_loss],\n",
    "            feed_dict={X: GT, Z: z_mb}\n",
    "        )\n",
    "\n",
    "        _, G_loss_curr = sess.run(\n",
    "           [G_solver, G_loss],\n",
    "           feed_dict={X: GT, Z: z_mb }\n",
    "            \n",
    "       )\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "                 .format(it, D_loss_curr, G_loss_curr))\n",
    "\n",
    "        samples = sess.run(G_sample, feed_dict={Z: z_mb})\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
